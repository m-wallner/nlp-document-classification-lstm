{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nlp-document-classification-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wallner/nlp-document-classification-lstm/blob/main/nlp-document-classification-lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JphRNLIAsHdw"
      },
      "source": [
        "# Natural language processing: Document classification using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCEhbDznEPat"
      },
      "source": [
        "## Project overview\n",
        "\n",
        "The goal of this project was to create a complete NLP pipeline for document classification using the pre-trained vectors of a word embedding model combined with an LSTM model.\n",
        "\n",
        "### Preprocessing\n",
        "First, the data set is loaded, all documents get tokenized, and a dictionary of vocabularies is created from the tokenized text, with tokens with low frequencies being excluded. Next, a lookup for the embeddings of all the words in the dictionary is created – this is an embedding matrix that maps the ID of each word to the respective pre-trained vector from the embedding model, which is GloVe with a vector length of 300 in this case. Words that are not found in the embedding model are replaced by randomly initialized vectors. The preprocessed and embedded data is then pickled to save time in future runs, and a PyTorch Dataset object is created for the training, validation and test set for optimized data loading during training and inference time.\n",
        "\n",
        "### Training the model\n",
        "The words in each tokenized document in a batch are turned into word IDs based on the embedding matrix and the respective pretrained word embeddings are fetched. Next, the LSTM model calculates the hidden states of each given document and uses the last hidden state as document embedding which is sent through a dropout layer with a dropout probability of 0.5, a linear decoder layer, and finally a Softmax layer to predict the probability distribution over the output classes.\n",
        "This model is then used for further experiments, with each experiment just applying a single change to the baseline architecture: The number of hidden dimensions is increased from 128 to 512, dropout is increased from 0.5 to 0.8, and finally a bidirectional LSTM is used, with each experiment reporting the results in terms of document classification accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQfVagHA-PM"
      },
      "source": [
        "## 1 Imports and data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbHFVqki2BgA"
      },
      "source": [
        "!pip install torchtext==0.8.1\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugzr5CnpyOuk"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn  import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "from torchtext.data import Field, Dataset, Example, BucketIterator\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQPj72zvygFr",
        "outputId": "952fe2b8-e5c4-431c-f8cd-94e757d46708"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define data paths\n",
        "data_path = '/content/gdrive/My Drive/Colab Notebooks/data/NLP/A1'\n",
        "labels_path = '/content/gdrive/My Drive/Colab Notebooks/data/NLP/A1/thedeep/thedeep.labels.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1NWgjtmtbHz"
      },
      "source": [
        "### 1.1 Loading thedeep dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "wx9rWjExyhyG",
        "outputId": "7102c4c7-f6f6-41f0-c2ba-403aee0bdcb0"
      },
      "source": [
        "# Load thedeep training dataset into Pandas dataframe\n",
        "thedeep_df_train = pd.read_csv(\n",
        "  os.path.join(data_path, 'thedeep/thedeep.medium.train.txt'),\n",
        "  sep=',',\n",
        "  names=['sentence_id', 'text', 'label'],\n",
        "  index_col=0,\n",
        "  skiprows=[0]\n",
        ")\n",
        "\n",
        "# Load thedeep validation dataset into Pandas dataframe\n",
        "thedeep_df_valid = pd.read_csv(\n",
        "  os.path.join(data_path, 'thedeep/thedeep.medium.validation.txt'),\n",
        "  sep=',',\n",
        "  names=['sentence_id', 'text', 'label'],\n",
        "  index_col=0,\n",
        "  skiprows=[0]\n",
        ")\n",
        "\n",
        "# Load thedeep test dataset into Pandas dataframe\n",
        "thedeep_df_test = pd.read_csv(\n",
        "  os.path.join(data_path, 'thedeep/thedeep.medium.test.txt'),\n",
        "  sep=',',\n",
        "  names=['sentence_id', 'text', 'label'],\n",
        "  index_col=0,\n",
        "  skiprows=[0]\n",
        ")\n",
        "\n",
        "# Show structure of thedeep dataset\n",
        "thedeep_df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentence_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28291</th>\n",
              "      <td>The primary reported needs for IDPs across the...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9695</th>\n",
              "      <td>Some 602 000 IDPs are now spread across the co...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7781</th>\n",
              "      <td>South Sudanese soldiers accused of raping at l...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31382</th>\n",
              "      <td>Since the beginning of 2017, 18 882 suspected/...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19919</th>\n",
              "      <td>The number of new suspected cholera cases in 2...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          text  label\n",
              "sentence_id                                                          \n",
              "28291        The primary reported needs for IDPs across the...      4\n",
              "9695         Some 602 000 IDPs are now spread across the co...      3\n",
              "7781         South Sudanese soldiers accused of raping at l...      9\n",
              "31382        Since the beginning of 2017, 18 882 suspected/...     11\n",
              "19919        The number of new suspected cholera cases in 2...     11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOmRIY3h2tGE"
      },
      "source": [
        "### 1.2 Basic information about thedeep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlzcX5mN3Oyo",
        "outputId": "9e31ad08-ce0e-4173-823a-e594795f63d8"
      },
      "source": [
        "# Load label captions\n",
        "labelcaptions = {}\n",
        "with open(labels_path) as fr:\n",
        "  for label in fr:\n",
        "    vals = label.strip().split(',')\n",
        "    labelcaptions[vals[1]] = int(vals[0])\n",
        "    \n",
        "# Show labels and corresponding numbers\n",
        "labelcaptions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Agriculture': 0,\n",
              " 'Cross': 1,\n",
              " 'Education': 2,\n",
              " 'Food': 3,\n",
              " 'Health': 4,\n",
              " 'Livelihood': 5,\n",
              " 'Logistic': 6,\n",
              " 'NFI': 7,\n",
              " 'Nutrition': 8,\n",
              " 'Protection': 9,\n",
              " 'Shelter': 10,\n",
              " 'WASH': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0KbzpvP3gOC",
        "outputId": "58d344ad-2803-437f-d216-2e8f589a35f8"
      },
      "source": [
        "# Show number of training samples per label\n",
        "thedeep_df_train['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     5419\n",
              "9     4618\n",
              "3     4341\n",
              "10    2553\n",
              "11    2178\n",
              "5     1712\n",
              "2     1278\n",
              "8     1207\n",
              "1     1066\n",
              "7     1054\n",
              "0      743\n",
              "6      430\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN4Nitt5X6dO",
        "outputId": "3703684c-bc5b-4331-d295-17ab25268aa7"
      },
      "source": [
        "# Show number of validation samples per label\n",
        "thedeep_df_valid['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     1196\n",
              "9      960\n",
              "3      954\n",
              "10     474\n",
              "11     463\n",
              "5      378\n",
              "2      300\n",
              "8      264\n",
              "1      232\n",
              "7      229\n",
              "0      168\n",
              "6       81\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOF15Wi8X64M",
        "outputId": "88158fb5-65f6-4b35-a9a7-6bd1e8e3e67f"
      },
      "source": [
        "# Show number of test samples per label\n",
        "thedeep_df_test['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     1181\n",
              "9      957\n",
              "3      944\n",
              "10     509\n",
              "11     484\n",
              "5      382\n",
              "2      283\n",
              "8      272\n",
              "1      223\n",
              "7      193\n",
              "0      177\n",
              "6       94\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34rlhNcTBWV7"
      },
      "source": [
        "## 2 Data preprocessing, word embedding and saving\n",
        "\n",
        "Just executed once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv0UcnO6up5l"
      },
      "source": [
        "### 2.1 Define torchtext.Field and apply preprocessing steps to thedeep dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv3pR8JA2cqA",
        "outputId": "949bec03-4f1f-4a87-a2ff-3f3f429f822a"
      },
      "source": [
        "# Define torchtext.Field objects for Tensor representation of data\n",
        "text_field = Field(tokenize='spacy', lower=True, batch_first=True)\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
        "fields = [('')]\n",
        "\n",
        "# Apply preprocessing to training, validation and test set\n",
        "text_train_pre = thedeep_df_train['text'].apply(lambda x: text_field.preprocess(x))\n",
        "text_valid_pre = thedeep_df_valid['text'].apply(lambda x: text_field.preprocess(x))\n",
        "text_test_pre = thedeep_df_test['text'].apply(lambda x: text_field.preprocess(x))\n",
        "\n",
        "text_train_pre"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentence_id\n",
              "28291    [the, primary, reported, needs, for, idps, acr...\n",
              "9695     [some, 602,  , 000, idps, are, now, spread, ac...\n",
              "7781     [south, sudanese, soldiers, accused, of, rapin...\n",
              "31382    [since, the, beginning, of, 2017, ,, 18, 882, ...\n",
              "19919    [the, number, of, new, suspected, cholera, cas...\n",
              "                               ...                        \n",
              "36292    [cholera, continues, to, spread, in, yemen, ,,...\n",
              "5566     [an, estimated, 165,000, children, are, expect...\n",
              "19676    [on, 3, march, 2017, ,, tropical, storm, enawo...\n",
              "29831    [the, presence, of, uxo, was, reported, in, 15...\n",
              "27747    [as, at, week, 27, (, july, 1, -, 7, ,, 2017, ...\n",
              "Name: text, Length: 26599, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFb3tzVY3E10"
      },
      "source": [
        "### 2.2 Load GloVe.6B.300d word embeddings, create dictionary and word embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaiMnXvXP65P",
        "outputId": "0eb6a7fd-f57e-4070-da01-22438b1a681f"
      },
      "source": [
        "# Load GloVe6B.300d word embedding - takes a LOOONG time - and build\n",
        "# GloVe-based vocabulary for all datasets\n",
        "text_field.build_vocab(text_train_pre, vectors='glove.6B.300d')\n",
        "text_field.build_vocab(text_valid_pre, vectors='glove.6B.300d')\n",
        "text_field.build_vocab(text_test_pre, vectors='glove.6B.300d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399998/400000 [00:37<00:00, 10858.42it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x-OnxuoZaB4",
        "outputId": "9ca1c8fc-db67-44ac-bb68-7b3b15654df5"
      },
      "source": [
        "# Checking total number of different words in corpus after preprocessing\n",
        "text_pre = [text_train_pre, text_valid_pre, text_test_pre]\n",
        "dictionary = {}\n",
        "for text in text_pre:\n",
        "  for doc in text:\n",
        "    for word in doc:\n",
        "      if word not in dictionary: dictionary[word] = 1\n",
        "      else: dictionary[word] += 1\n",
        "\n",
        "print(f'Length of dictionary: {len(dictionary)} words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of dictionary: 48817 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C2rE80Vu1yk"
      },
      "source": [
        "### 2.3 Initialize words not found in vocabulary with random values from a normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqP4yzs0N9Oc",
        "outputId": "84e38c38-a7ae-4d65-f049-b762268d7215"
      },
      "source": [
        "# Get torchtext.vocab instance and show the structure of the tensor.\n",
        "# The whole corpus is in one big tensor.\n",
        "text_field.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVu7axT5KSh_",
        "outputId": "43a3f61b-b661-4694-ee73-d3366aaf7159"
      },
      "source": [
        "# Turn all words which were not contained in Glove vocabulary from zero vectors into random\n",
        "# vectors with a normal distribution\n",
        "\n",
        "# Define zero vector to compare other vectors to\n",
        "zero_tensor = torch.zeros_like(text_field.vocab.vectors[0])\n",
        "\n",
        "# Turn zero vectors in vocabulary.vectors to random vectors with std = 1\n",
        "counter = 0\n",
        "for i, vector in enumerate(text_field.vocab.vectors):\n",
        "  if torch.all(torch.eq(vector, zero_tensor)):\n",
        "    text_field.vocab.vectors[i] = torch.randn_like(zero_tensor)\n",
        "    counter += 1\n",
        "\n",
        "print(f'{counter} new words initialized randomly with normally distributed values \\n')\n",
        "\n",
        "# Show updated tensor without zero-vectors\n",
        "text_field.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4538 new words initialized randomly with normally distributed values \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3601, -0.4453,  1.6286,  ..., -0.9567,  0.9818,  0.1765],\n",
              "        [-2.4346, -0.3123, -0.9448,  ..., -2.1006, -0.7532,  0.8999],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 1.0273,  0.0028, -0.3037,  ..., -2.4012, -0.5784, -0.6563],\n",
              "        [ 0.3459,  0.4757,  0.1960,  ...,  0.8434,  2.1771,  0.0535],\n",
              "        [-0.3181, -1.0090,  0.6965,  ..., -0.9082,  0.0988, -1.5894]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1LIzlzAsySE"
      },
      "source": [
        "### 2.4 Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDQPlFglN_hm"
      },
      "source": [
        "# Pickle preprocessed and embedded data\n",
        "with open(os.path.join(data_path, 'text_field.pickle'), 'wb') as f:\n",
        "    pickle.dump(text_field, f)\n",
        "with open(os.path.join(data_path, 'label_field.pickle'), 'wb') as f:\n",
        "    pickle.dump(label_field, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0xWX0nEvSfM"
      },
      "source": [
        "## 3 Load preprocessed and embedded data and construct Dataset object from pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXrElSQNkFIm",
        "outputId": "d8b80ef9-4749-47b9-efe3-e6300240be72"
      },
      "source": [
        "# Load preprocessed and embedded data\n",
        "with open(os.path.join(data_path, 'text_field.pickle'), 'rb') as f:\n",
        "    text_field = pickle.load(f)\n",
        "with open(os.path.join(data_path, 'label_field.pickle'), 'rb') as f:\n",
        "    label_field = pickle.load(f)\n",
        "\n",
        "# Define torchtext Dataset class to load pandas DataFrame\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, df:pd.DataFrame, fields:list):\n",
        "        super(DataFrameDataset, self).__init__(\n",
        "            [Example.fromlist(list(r), fields) for i, r in df.iterrows()], fields\n",
        "        )\n",
        "\n",
        "# Construct DataFrameDataset for all datasets\n",
        "fields = (('text', text_field), ('label', label_field))\n",
        "train_dataset = DataFrameDataset(df=thedeep_df_train, fields=fields)\n",
        "valid_dataset = DataFrameDataset(df=thedeep_df_valid, fields=fields)\n",
        "test_dataset = DataFrameDataset(df=thedeep_df_test, fields=fields)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxJOH8-TfNMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff19fa82-356d-457c-83d5-e25a7956bccf"
      },
      "source": [
        "# Example sentence in torchtext.data.Example object\n",
        "train_dataset[0].text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'primary',\n",
              " 'reported',\n",
              " 'needs',\n",
              " 'for',\n",
              " 'idps',\n",
              " 'across',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'of',\n",
              " 'libya',\n",
              " 'were',\n",
              " 'access',\n",
              " 'to',\n",
              " 'food',\n",
              " ',',\n",
              " 'health',\n",
              " 'services',\n",
              " 'and',\n",
              " 'shelter',\n",
              " '.',\n",
              " 'the',\n",
              " 'main',\n",
              " 'issues',\n",
              " 'related',\n",
              " 'to',\n",
              " 'the',\n",
              " 'above',\n",
              " '-',\n",
              " 'mentioned',\n",
              " 'needs',\n",
              " 'are',\n",
              " 'that',\n",
              " 'goods',\n",
              " 'are',\n",
              " 'too',\n",
              " 'expensive',\n",
              " 'and',\n",
              " 'therefore',\n",
              " 'idps',\n",
              " 'have',\n",
              " 'limit',\n",
              " 'access',\n",
              " '.',\n",
              " 'other',\n",
              " 'issues',\n",
              " 'cited',\n",
              " 'for',\n",
              " 'access',\n",
              " 'to',\n",
              " 'health',\n",
              " 'included',\n",
              " 'irregular',\n",
              " 'supply',\n",
              " 'of',\n",
              " 'medicines',\n",
              " 'and',\n",
              " 'low',\n",
              " 'quality',\n",
              " 'of',\n",
              " 'available',\n",
              " 'health',\n",
              " 'services',\n",
              " 'due',\n",
              " 'to',\n",
              " 'overcrowded',\n",
              " 'facilities',\n",
              " ',',\n",
              " 'lack',\n",
              " 'of',\n",
              " 'medical',\n",
              " 'staff',\n",
              " 'and',\n",
              " 'a',\n",
              " 'diminished',\n",
              " 'availability',\n",
              " 'of',\n",
              " 'female',\n",
              " 'doctors',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBNxhJa9B8Qm"
      },
      "source": [
        "## 4 Definition of LSTM model and training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOw3CQvfdDtR"
      },
      "source": [
        "### 4.1 Define LSTM model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVn-x56rpx6t"
      },
      "source": [
        "class ClassificationRNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_class, dropout, bidirectional=False):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_class = num_class\n",
        "    self.bidirectional = bidirectional\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(input_size=embed_dim,\n",
        "                    hidden_size=hidden_dim,\n",
        "                    num_layers=1,\n",
        "                    bidirectional=bidirectional)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "  def forward(self, text, lengths):\n",
        "    if self.bidirectional:\n",
        "      embedded = self.embedding(text)\n",
        "      packed_embedded = pack_padded_sequence(embedded, lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
        "      packed_output, (h_t, c_t) = self.lstm(packed_embedded)\n",
        "      hidden = self.dropout(h_t[-1])\n",
        "      # Use LSTM's last hidden state as input for FC layer\n",
        "      text_features = self.fc(hidden)\n",
        "    else:\n",
        "      embedded = self.embedding(text)\n",
        "      packed_embedded = pack_padded_sequence(embedded, lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
        "      packed_output, (h_t, c_t) = self.lstm(packed_embedded)\n",
        "      hidden = self.dropout(h_t[-1])\n",
        "      # Use LSTM's last hidden state as input for FC layer\n",
        "      text_features = self.fc(hidden)\n",
        "\n",
        "    return text_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gTAQR_C4pjf"
      },
      "source": [
        "### 4.2 collate_fn function for PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwsN1aWugWmr"
      },
      "source": [
        "# Data batching for RNN\n",
        "# Text entries have different lengths => use custom function to generate data\n",
        "# batches (texts, lengths, labels), then pass it to collate_fn in Pytorch DataLoader\n",
        "\n",
        "def generate_batch_rnn(batch):\n",
        "  texts, lengths, labels = [], [], []\n",
        "  \n",
        "  for sample in batch:\n",
        "    sample_list = [vocabulary[word] for word in sample.text]\n",
        "    texts.append(torch.LongTensor(sample_list))\n",
        "    lengths.append(len(sample_list))\n",
        "    labels.append(sample.label)\n",
        "\n",
        "  # Even out text lengths\n",
        "  texts = pad_sequence(texts, batch_first=True)\n",
        "  lengths = torch.tensor(lengths)\n",
        "  labels = torch.tensor (labels)\n",
        "\n",
        "  return texts, lengths, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW0MDGzidtvh"
      },
      "source": [
        "### 4.3 Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOerlAD3Lew-"
      },
      "source": [
        "def train(model, optimizer, criterion, train_data, valid_data, test_data, epochs):\n",
        "  valid_loss, best_val_loss = 10001, 10000\n",
        "  counter = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = 0, 0\n",
        "    \n",
        "    for i, (text, lengths, labels) in enumerate(train_data):\n",
        "      optimizer.zero_grad()\n",
        "      text, lengths, labels = text.to(device), lengths.to(device), labels.to(device)\n",
        "      output = model(text, lengths)\n",
        "      loss = criterion(output, labels)\n",
        "      train_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_acc = train_acc + (output.argmax(1) == labels).sum().item() / len(text)\n",
        "\n",
        "    train_loss = train_loss / len(train_data)\n",
        "    train_acc = train_acc / len(train_data)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1} | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | {secs:.1f} sec')\n",
        "    if valid_data and (epoch+1) % 5 == 0: \n",
        "      valid_loss = test(model, criterion, valid_data, mode='valid')[0]\n",
        "      counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if valid_loss < best_val_loss:\n",
        "      with open('model.pt', 'wb') as f:\n",
        "        torch.save(model, f) # save current state of model\n",
        "        best_val_loss = valid_loss\n",
        "        counter = 0 # reset counter if a new best validation loss was found\n",
        "    \n",
        "    if counter == 3:\n",
        "      # break for loop and stop training\n",
        "      print(f'Early stopping triggered.')\n",
        "      break\n",
        "\n",
        "  with open('model.pt', 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "  # Test best model on test dataset\n",
        "  if test_data: test(model, criterion, test_data, mode='test')\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def test(model, criterion, data, mode='test'):\n",
        "  loss, acc = 0, 0\n",
        "\n",
        "  for text, lengths, labels in data:\n",
        "    text, lengths, labels = text.to(device), lengths.to(device), labels.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      output = model(text, lengths)\n",
        "      loss = criterion(output, labels)\n",
        "      loss += loss.item()\n",
        "      acc = acc + (output.argmax(1) == labels).sum().item() / len(text)\n",
        "  \n",
        "  loss = loss / len(data)\n",
        "  acc = acc / len(data)\n",
        "  if mode == 'test':\n",
        "    print('Testing best model...\\n')\n",
        "    print(f'\\nTest | Loss: {loss:.4f} | Acc: {acc:.4f}\\n')\n",
        "  else:\n",
        "    print(f'\\nValidation | Loss: {loss:.4f} | Acc: {acc:.4f}\\n')\n",
        "\n",
        "  return loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sF_u9xGQoqG"
      },
      "source": [
        "## 5 Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTZzlqttRQ0p"
      },
      "source": [
        "### 5.1 Standard parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLxzPe5biD9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24e8b40-1358-481b-dd79-167e1d3753f8"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(text_field.vocab.vectors)\n",
        "EMBED_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.5\n",
        "N_CLASSES = len(labelcaptions)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ClassificationRNNModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_CLASSES, DROPOUT).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_rnn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "\n",
        "model_trained = train(model, optimizer, criterion, train_loader, valid_loader, test_loader, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Loss: 2.1578 | Acc: 0.2773 | 8.0 sec\n",
            "Epoch: 2 | Loss: 1.7314 | Acc: 0.4696 | 8.0 sec\n",
            "Epoch: 3 | Loss: 1.6218 | Acc: 0.4994 | 8.0 sec\n",
            "Epoch: 4 | Loss: 1.4128 | Acc: 0.5732 | 8.0 sec\n",
            "Epoch: 5 | Loss: 1.2711 | Acc: 0.6147 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0614 | Acc: 0.6062\n",
            "\n",
            "Epoch: 6 | Loss: 1.1772 | Acc: 0.6387 | 8.0 sec\n",
            "Epoch: 7 | Loss: 1.0967 | Acc: 0.6561 | 8.0 sec\n",
            "Epoch: 8 | Loss: 1.0628 | Acc: 0.6638 | 8.0 sec\n",
            "Epoch: 9 | Loss: 0.9951 | Acc: 0.6808 | 8.0 sec\n",
            "Epoch: 10 | Loss: 0.9449 | Acc: 0.6950 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0364 | Acc: 0.6190\n",
            "\n",
            "Epoch: 11 | Loss: 0.8934 | Acc: 0.7078 | 8.0 sec\n",
            "Epoch: 12 | Loss: 0.8476 | Acc: 0.7216 | 8.0 sec\n",
            "Epoch: 13 | Loss: 0.8080 | Acc: 0.7289 | 8.0 sec\n",
            "Epoch: 14 | Loss: 0.7633 | Acc: 0.7383 | 8.0 sec\n",
            "Epoch: 15 | Loss: 0.7419 | Acc: 0.7450 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0365 | Acc: 0.6171\n",
            "\n",
            "Epoch: 16 | Loss: 0.7032 | Acc: 0.7549 | 8.0 sec\n",
            "Epoch: 17 | Loss: 0.6726 | Acc: 0.7607 | 8.0 sec\n",
            "Epoch: 18 | Loss: 0.6404 | Acc: 0.7687 | 8.0 sec\n",
            "Epoch: 19 | Loss: 0.6090 | Acc: 0.7774 | 8.0 sec\n",
            "Epoch: 20 | Loss: 0.5943 | Acc: 0.7800 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0633 | Acc: 0.6086\n",
            "\n",
            "Epoch: 21 | Loss: 0.5773 | Acc: 0.7840 | 8.0 sec\n",
            "Epoch: 22 | Loss: 0.5556 | Acc: 0.7882 | 8.0 sec\n",
            "Epoch: 23 | Loss: 0.5387 | Acc: 0.7938 | 8.0 sec\n",
            "Epoch: 24 | Loss: 0.5300 | Acc: 0.7929 | 8.0 sec\n",
            "Epoch: 25 | Loss: 0.5271 | Acc: 0.7957 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0829 | Acc: 0.6032\n",
            "\n",
            "Early stopping triggered.\n",
            "\n",
            "Test | Loss: 0.0325 | Acc: 0.6180\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSNRiNpTRw1v"
      },
      "source": [
        "### 5.2 Experiment 1: Increased number of LSTM's hidden dimensions: 128 => 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgWLk0Nk1HzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0c4665-f992-4ff9-d281-3036af3eef4a"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(text_field.vocab.vectors)\n",
        "EMBED_DIM = 300\n",
        "HIDDEN_DIM = 512 # Increased from 128 to 512\n",
        "DROPOUT = 0.5\n",
        "N_CLASSES = len(labelcaptions)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = ClassificationRNNModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_CLASSES, DROPOUT).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_rnn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "\n",
        "# Define TensorBoard summary writer for simple network\n",
        "writer = SummaryWriter('runs/thedeep_rnn')\n",
        "\n",
        "model_trained = train(model, optimizer, criterion, train_loader, valid_loader, test_loader, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch: 1 | Loss: 2.1112 | Acc: 0.3182 | 19.0 sec\n",
            "Epoch: 2 | Loss: 1.9899 | Acc: 0.3541 | 19.0 sec\n",
            "Epoch: 3 | Loss: 1.5493 | Acc: 0.5318 | 19.0 sec\n",
            "Epoch: 4 | Loss: 1.2765 | Acc: 0.6111 | 19.0 sec\n",
            "Epoch: 5 | Loss: 1.1465 | Acc: 0.6413 | 19.0 sec\n",
            "\n",
            "Validation | Loss: 0.0350 | Acc: 0.6281\n",
            "\n",
            "Epoch: 6 | Loss: 1.0410 | Acc: 0.6657 | 19.0 sec\n",
            "Epoch: 7 | Loss: 0.9442 | Acc: 0.6894 | 19.0 sec\n",
            "Epoch: 8 | Loss: 0.8625 | Acc: 0.7089 | 19.0 sec\n",
            "Epoch: 9 | Loss: 0.7935 | Acc: 0.7257 | 19.0 sec\n",
            "Epoch: 10 | Loss: 0.7180 | Acc: 0.7456 | 19.0 sec\n",
            "\n",
            "Validation | Loss: 0.0253 | Acc: 0.6234\n",
            "\n",
            "Epoch: 11 | Loss: 0.6587 | Acc: 0.7572 | 19.0 sec\n",
            "Epoch: 12 | Loss: 0.6048 | Acc: 0.7725 | 19.0 sec\n",
            "Epoch: 13 | Loss: 0.5553 | Acc: 0.7816 | 19.0 sec\n",
            "Epoch: 14 | Loss: 0.5291 | Acc: 0.7854 | 19.0 sec\n",
            "Epoch: 15 | Loss: 0.5068 | Acc: 0.7897 | 19.0 sec\n",
            "\n",
            "Validation | Loss: 0.0168 | Acc: 0.6069\n",
            "\n",
            "Epoch: 16 | Loss: 0.4840 | Acc: 0.7924 | 19.0 sec\n",
            "Epoch: 17 | Loss: 0.4615 | Acc: 0.7958 | 19.0 sec\n",
            "Epoch: 18 | Loss: 0.4505 | Acc: 0.7980 | 19.0 sec\n",
            "Epoch: 19 | Loss: 0.4407 | Acc: 0.7994 | 19.0 sec\n",
            "Epoch: 20 | Loss: 0.4300 | Acc: 0.8006 | 19.0 sec\n",
            "\n",
            "Validation | Loss: 0.0212 | Acc: 0.6029\n",
            "\n",
            "Epoch: 21 | Loss: 0.4196 | Acc: 0.8044 | 19.0 sec\n",
            "Epoch: 22 | Loss: 0.4085 | Acc: 0.8053 | 19.0 sec\n",
            "Epoch: 23 | Loss: 0.4024 | Acc: 0.8048 | 19.0 sec\n",
            "Epoch: 24 | Loss: 0.4037 | Acc: 0.8068 | 19.0 sec\n",
            "Epoch: 25 | Loss: 0.3997 | Acc: 0.8056 | 19.0 sec\n",
            "\n",
            "Validation | Loss: 0.0386 | Acc: 0.5979\n",
            "\n",
            "Epoch: 26 | Loss: 0.3836 | Acc: 0.8073 | 19.0 sec\n",
            "Epoch: 27 | Loss: 0.3733 | Acc: 0.8101 | 19.0 sec\n",
            "Epoch: 28 | Loss: 0.3791 | Acc: 0.8070 | 19.0 sec\n",
            "Epoch: 29 | Loss: 0.3798 | Acc: 0.8069 | 19.0 sec\n",
            "Epoch: 30 | Loss: 0.3667 | Acc: 0.8096 | 19.0 sec\n",
            "\n",
            "Validation | Loss: 0.0447 | Acc: 0.5928\n",
            "\n",
            "Early stopping triggered.\n",
            "\n",
            "Test | Loss: 0.0598 | Acc: 0.5954\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LW-lPhCVLXk"
      },
      "source": [
        "### 5.3 Experiment 2: Increased dropout rate: 0.5 => 0.8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy6hNx_d8sw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272f3136-ee60-406b-c770-a75b03de231a"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(text_field.vocab.vectors)\n",
        "EMBED_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.8 # Increased dropout rate from 0.5 to 0.8\n",
        "N_CLASSES = len(labelcaptions)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = ClassificationRNNModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_CLASSES, DROPOUT).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_rnn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "\n",
        "# Define TensorBoard summary writer for simple network\n",
        "writer = SummaryWriter('runs/thedeep_rnn')\n",
        "\n",
        "model_trained = train(model, optimizer, criterion, train_loader, valid_loader, test_loader, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch: 1 | Loss: 2.2211 | Acc: 0.2560 | 8.0 sec\n",
            "Epoch: 2 | Loss: 1.9976 | Acc: 0.3633 | 8.0 sec\n",
            "Epoch: 3 | Loss: 1.7547 | Acc: 0.4683 | 8.0 sec\n",
            "Epoch: 4 | Loss: 1.5591 | Acc: 0.5379 | 8.0 sec\n",
            "Epoch: 5 | Loss: 1.4916 | Acc: 0.5624 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0395 | Acc: 0.5581\n",
            "\n",
            "Epoch: 6 | Loss: 1.3432 | Acc: 0.6111 | 8.0 sec\n",
            "Epoch: 7 | Loss: 1.2752 | Acc: 0.6259 | 8.0 sec\n",
            "Epoch: 8 | Loss: 1.2070 | Acc: 0.6390 | 8.0 sec\n",
            "Epoch: 9 | Loss: 1.1719 | Acc: 0.6446 | 8.0 sec\n",
            "Epoch: 10 | Loss: 1.1240 | Acc: 0.6563 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0472 | Acc: 0.6027\n",
            "\n",
            "Epoch: 11 | Loss: 1.0733 | Acc: 0.6676 | 8.0 sec\n",
            "Epoch: 12 | Loss: 1.0327 | Acc: 0.6751 | 8.0 sec\n",
            "Epoch: 13 | Loss: 1.0042 | Acc: 0.6855 | 8.0 sec\n",
            "Epoch: 14 | Loss: 0.9715 | Acc: 0.6913 | 8.0 sec\n",
            "Epoch: 15 | Loss: 0.9434 | Acc: 0.6985 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0566 | Acc: 0.6038\n",
            "\n",
            "Epoch: 16 | Loss: 0.9150 | Acc: 0.7051 | 8.0 sec\n",
            "Epoch: 17 | Loss: 0.8878 | Acc: 0.7096 | 8.0 sec\n",
            "Epoch: 18 | Loss: 0.8758 | Acc: 0.7163 | 8.0 sec\n",
            "Epoch: 19 | Loss: 0.8330 | Acc: 0.7239 | 8.0 sec\n",
            "Epoch: 20 | Loss: 0.8120 | Acc: 0.7299 | 8.0 sec\n",
            "\n",
            "Validation | Loss: 0.0525 | Acc: 0.6060\n",
            "\n",
            "Early stopping triggered.\n",
            "\n",
            "Test | Loss: 0.0325 | Acc: 0.5614\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOeLjC9dVbGA"
      },
      "source": [
        "### 5.4 Experiment 3: Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqgha5_MyGKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2201916e-9274-4093-ada1-01fb324902ad"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(text_field.vocab.vectors)\n",
        "EMBED_DIM = 300\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.5\n",
        "N_CLASSES = len(labelcaptions)\n",
        "BIDIRECTIONAL = True # Bidirectional LSTM\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = ClassificationRNNModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_CLASSES, DROPOUT, BIDIRECTIONAL).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_rnn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_rnn)\n",
        "\n",
        "# Define TensorBoard summary writer for simple network\n",
        "writer = SummaryWriter('runs/thedeep_rnn')\n",
        "\n",
        "model_trained = train(model, optimizer, criterion, train_loader, valid_loader, test_loader, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch: 1 | Loss: 2.0724 | Acc: 0.3193 | 12.0 sec\n",
            "Epoch: 2 | Loss: 1.6556 | Acc: 0.4987 | 12.0 sec\n",
            "Epoch: 3 | Loss: 1.3980 | Acc: 0.5826 | 12.0 sec\n",
            "Epoch: 4 | Loss: 1.2619 | Acc: 0.6196 | 12.0 sec\n",
            "Epoch: 5 | Loss: 1.1575 | Acc: 0.6451 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0469 | Acc: 0.6020\n",
            "\n",
            "Epoch: 6 | Loss: 1.0970 | Acc: 0.6577 | 12.0 sec\n",
            "Epoch: 7 | Loss: 1.0361 | Acc: 0.6700 | 12.0 sec\n",
            "Epoch: 8 | Loss: 0.9758 | Acc: 0.6861 | 12.0 sec\n",
            "Epoch: 9 | Loss: 0.9252 | Acc: 0.6964 | 12.0 sec\n",
            "Epoch: 10 | Loss: 0.8771 | Acc: 0.7100 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0459 | Acc: 0.6138\n",
            "\n",
            "Epoch: 11 | Loss: 0.8336 | Acc: 0.7214 | 12.0 sec\n",
            "Epoch: 12 | Loss: 0.8317 | Acc: 0.7243 | 12.0 sec\n",
            "Epoch: 13 | Loss: 0.7590 | Acc: 0.7447 | 12.0 sec\n",
            "Epoch: 14 | Loss: 0.7232 | Acc: 0.7505 | 12.0 sec\n",
            "Epoch: 15 | Loss: 0.6880 | Acc: 0.7585 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0284 | Acc: 0.6041\n",
            "\n",
            "Epoch: 16 | Loss: 0.6618 | Acc: 0.7672 | 12.0 sec\n",
            "Epoch: 17 | Loss: 0.6397 | Acc: 0.7696 | 12.0 sec\n",
            "Epoch: 18 | Loss: 0.6148 | Acc: 0.7753 | 12.0 sec\n",
            "Epoch: 19 | Loss: 0.5973 | Acc: 0.7788 | 12.0 sec\n",
            "Epoch: 20 | Loss: 0.5725 | Acc: 0.7855 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0238 | Acc: 0.5928\n",
            "\n",
            "Epoch: 21 | Loss: 0.5529 | Acc: 0.7893 | 12.0 sec\n",
            "Epoch: 22 | Loss: 0.5397 | Acc: 0.7940 | 12.0 sec\n",
            "Epoch: 23 | Loss: 0.5342 | Acc: 0.7941 | 12.0 sec\n",
            "Epoch: 24 | Loss: 0.5192 | Acc: 0.7955 | 12.0 sec\n",
            "Epoch: 25 | Loss: 0.5091 | Acc: 0.7967 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0127 | Acc: 0.5947\n",
            "\n",
            "Epoch: 26 | Loss: 0.4972 | Acc: 0.8018 | 12.0 sec\n",
            "Epoch: 27 | Loss: 0.4902 | Acc: 0.8012 | 12.0 sec\n",
            "Epoch: 28 | Loss: 0.4781 | Acc: 0.8033 | 12.0 sec\n",
            "Epoch: 29 | Loss: 0.4740 | Acc: 0.8052 | 12.0 sec\n",
            "Epoch: 30 | Loss: 0.4687 | Acc: 0.8057 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0310 | Acc: 0.5906\n",
            "\n",
            "Epoch: 31 | Loss: 0.4599 | Acc: 0.8060 | 12.0 sec\n",
            "Epoch: 32 | Loss: 0.4838 | Acc: 0.8034 | 12.0 sec\n",
            "Epoch: 33 | Loss: 0.4714 | Acc: 0.8020 | 12.0 sec\n",
            "Epoch: 34 | Loss: 0.4438 | Acc: 0.8085 | 12.0 sec\n",
            "Epoch: 35 | Loss: 0.4387 | Acc: 0.8114 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0391 | Acc: 0.5866\n",
            "\n",
            "Epoch: 36 | Loss: 0.4384 | Acc: 0.8117 | 12.0 sec\n",
            "Epoch: 37 | Loss: 0.4342 | Acc: 0.8103 | 12.0 sec\n",
            "Epoch: 38 | Loss: 0.4226 | Acc: 0.8130 | 12.0 sec\n",
            "Epoch: 39 | Loss: 0.4286 | Acc: 0.8091 | 12.0 sec\n",
            "Epoch: 40 | Loss: 0.4256 | Acc: 0.8102 | 12.0 sec\n",
            "\n",
            "Validation | Loss: 0.0191 | Acc: 0.5866\n",
            "\n",
            "Early stopping triggered.\n",
            "\n",
            "Test | Loss: 0.0615 | Acc: 0.5827\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Ncv-TwSe_T"
      },
      "source": [
        "**Main sources:**\n",
        "\n",
        "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0\n",
        "\n",
        "https://www.kaggle.com/swarnabha/pytorch-text-classification-torchtext-lstm\n"
      ]
    }
  ]
}